---
permalink: /work-with-us/
title: "ğŸ¤ Work with Us"
author_profile: true
redirect_from:
    - /work-with-us/
    - /work-with-us.html
---

If interested in working with us, have a read of this page, to see if you

-   ğŸŒˆ buy our research belief,
-   ğŸ“š are interested in the research topics,
-   ğŸŒŸ consider yourself a good fit for the expected qualities,
-   ğŸ¯ and love what outcome you can expect in such an experience.

We welcome talented students and researchers at all levels (undergraduate, master, PhD, post-doc) and from all backgrounds (computer science, neuroscience, physics, mathematics, etc.).

## ğŸŒˆ Research Belief

We work across intersections of some of the most exciting topics related to intelligence (biological neurons & brains, new artificial neural networks, dissected transformers, next-generation GPUs, neuromorphic devices & systems, emerging memories, and etc.). Most importantly, we look at things in the most first-principled way (from a most fundamental level). Thus explains why the above seemingly diverse topics form a coherent whole in our research:

> We aim to discover and understand the fundamental elements of intelligence, so as to build the next-generation learning systems (for the future 10+ years ğŸš€).

## ğŸ“š Research Topics

Below lists the topics that we are currently most excited about and actively working on (being updated regularly):

-   âš›ï¸ The "atom" of learning. We have been working on a learning rule that internally we called "3Q", because the learning rule can be described by three simple equations of basic operations like addition and multiplication. It has the best biological plausibility weâ€™ve ever seen (thus potentially the best candidate for designing next-generation AI acceleration hardware). However, we have not yet quite figured out its learning capacity. What is the learning dynamic of 3Q? What are the "learning guarantees" of 3Q? How would 3Q (or can it) implement key computation motifs like gating, attention, etc.? (The work is not published yet, so apologies for not more details here.)
-   ğŸŒ€ The "essence" of self-attention. Self-attention has demonstrated to be the most expressive architecture that essentially opens up the era of "large models". We want to understand the "essence" of self-attention. For example, we see it is essentially an "associative memory" (see [our ICLR paper](https://proceedings.mlr.press/v162/millidge22a/millidge22a.pdf)), and it is related to the simplest problem in machine learning, linear regression (see [here](https://c16mftang.github.io/attention.html), particularly, [Mufeng](https://c16mftang.github.io/index.html) spotted this). So what is the "essence" of self-attention? Would understanding this unlock better self-attention, with simpler operators that is more biological plausible or/and hardware friendly? Or would understanding this inspire alternative mechanisms of self-attention? 
-   ğŸ¤” Better self-attention from better biological associative memory? Since we know self-attention can be seen as an "associative memory" from [our ICLR paper](https://proceedings.mlr.press/v162/millidge22a/millidge22a.pdf), [this nature paper](https://www.nature.com/articles/s41586-024-08392-y) talks about a new type of associative memory, would it inform better alternative of self-attention?
<!-- -   Neural networks as memories, in place of RAG. We learned through painful process of deploy AI in production that contextualization is the most important and challenging part. The current solution, RAG, is not working well nor elegant. We know neural networks can be made to as memories, would them be good enough  -->

More to come... But you should get a taste of our research here.

## ğŸŒŸ Expected Qualities

ğŸŒŒ **"Belief-wise": first principle believer.**
You should be a strong believer in first principles: meaning you have a strong curiosity in asking "but fundamentally why and how? ğŸ¤“". You enjoy such a process of deep thinking and exploring to understand the essence of things (in science, technology, even also in life). You enjoy this because you believe thatâ€™s the only way to make sense of things and create unique value!
ğŸ’¡ Linked to this, you should be an honest person: honest with people, with science and technology, and most importantly, honest with yourself.

ğŸ§© **"Intelligence-wise": you should either be able to "handle complexity" or can "make things simple".**
I see broadly two kinds of talents, and you can be either one of them (i.e., don't have to be both):

-   _Able to "handle complexity"._ Youâ€™re just good at working with symbols, and youâ€™re good at math or physics (demonstrated by your transcripts). In this case, you donâ€™t even have to be good at coding for now. But of course, you should consider coding as a tool and have a passion for learning it (at least some basics).
-   _Able to "make things simple"._ Youâ€™re eager to understand things in the deepest way, which often means the simplest way of explaining. You can demonstrate this by explaining a new concept/idea to a general audience. You enjoy explaining things and can make the explanation so simple and intuitive that it becomes a pleasant experience for both you and your audience. Youâ€™re normally good at coding in this case (or should be able to get good at coding soon) due to the particular way your brain approaches things.

ğŸŒŸ I myself am unfortunately not the first type but fortunate to be the second type.

ğŸŒ We will have intensive intentional collaboration, so you should be confident in your English or ready to improve in a short time.

ğŸŒ± You expect the time to be a pleasant intellectual exercise; you are curious, eager to explore, and learn.

## ğŸ¯ Expected Outcomes

**We produce "deep" work (papers, posts, repos, or anything) with profound impacts**, rather than "shallow" work with short-term aims (for example, just to have more papers accepted).
But "deep" papers do not necessarily mean "difficult" papers. With the right insights, we had experience before that we produced papers with good impact and are accepted in top AI conferences with 2-3 weeks' effort. (Of course, these are quite rare cases, normally deep work takes energy, enthusiasm, and time.)

**Should I expect myself to gain industrial or research qualities from the experience?**
We consider our research demands very general capability and thus train you to develop such qualities: understanding and solving problems from the fundamentals. 
Such capability should be desired in both industrial and research contexts.
But for those industrial careers requiring very specific technical skills, we are not aligned.
So overall, we consider the outcome to be leaning towards a research-type of career.

ğŸ‰ Congrats in making it to the end! You are now ready to [reach out to us](https://yuhangsong.github.io/contact/), including a brief self-introduction and why youâ€™re interested in working with us.